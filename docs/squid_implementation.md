# Реализация индексатора на базе Squid SDK

## 1. Общий обзор

В рамках проекта realtime-explorer мы реализовали индексатор на базе Squid SDK, который выполняет следующие функции:
- Подключается к QF TestNet через WebSocket
- Получает новые блоки в реальном времени
- Извлекает из блоков информацию о:
  - Самих блоках (хеш, номер, временная метка и т.д.)
  - Транзакциях (extrinsics)
  - Событиях (events)
- Сохраняет данные в PostgreSQL
- Поддерживает ограничение на количество хранимых блоков (1000)

## 2. Архитектура индексатора

```
+-------------------+        +-------------------+        +-------------------+
|                   |        |                   |        |                   |
|   QF TestNet      +------->+   Squid SDK       +------->+   PostgreSQL      |
|   (wss endpoint)  |        |   Индексатор      |        |   (Explorer DB)   |
|                   |        |                   |        |                   |
+-------------------+        +-------------------+        +-------------------+
```

### Основные компоненты:
- **processor.ts** - настройка подключения к блокчейну, выбор событий для индексации
- **main.ts** - основная логика обработки данных блокчейна
- **config.ts** - настройки, включая ограничение на количество блоков
- **db/cleanup.ts** - логика удаления старых блоков при превышении лимита

## 3. Механизм ограничения данных

Для оптимизации использования ресурсов мы реализовали механизм ограничения количества блоков в базе данных:

1. В конфигурации установлен лимит в 1000 блоков (`MAX_BLOCKS = 1000`)
2. После обработки каждого батча блоков запускается функция очистки
3. Функция проверяет текущее количество блоков в базе
4. Если количество превышает лимит, удаляются самые старые блоки (в порядке возрастания номера)
5. Удаление проходит в правильном порядке для соблюдения целостности:
   - Сначала удаляются события, связанные с блоками
   - Затем удаляются транзакции, связанные с блоками
   - В конце удаляются сами блоки

### Пример логики очистки:
```typescript
if (blockCount > MAX_BLOCKS) {
  // Найти самые старые блоки для удаления
  const blocksToDeleteCount = blockCount - MAX_BLOCKS
  
  // Получить самые старые блоки
  const oldestBlocks = await ctx.store.find(Block, {
    order: { number: 'ASC' },
    take: blocksToDeleteCount
  })
  
  // Получить ID блоков для удаления
  const blockIds = oldestBlocks.map((block: Block) => block.id)
  
  // Удалить связанные данные и сами блоки
  await ctx.store.remove(events)
  await ctx.store.remove(transactions)
  await ctx.store.remove(oldestBlocks)
}
```

## 4. Схема данных

Для хранения данных используются следующие сущности:

### Block (блок)
- **id**: хеш блока (primary key)
- **number**: номер блока
- **hash**: хеш блока
- **timestamp**: время создания
- **validator**: адрес валидатора
- **status**: статус (например, "finalized")
- **size**: размер блока

### Transaction (транзакция)
- **id**: уникальный ID транзакции
- **block**: связь с блоком
- **timestamp**: время транзакции
- **from**: отправитель (связь с аккаунтом)
- **to**: получатель (связь с аккаунтом)
- **amount**: сумма
- **fee**: комиссия
- **status**: статус транзакции
- **type**: тип транзакции
- **data**: дополнительные данные (JSON)

### Event (событие)
- **id**: уникальный ID события
- **block**: связь с блоком
- **transaction**: связь с транзакцией (если применимо)
- **section**: модуль события (например, "Balances")
- **method**: тип события (например, "Transfer")
- **data**: данные события (JSON)

### Account (аккаунт)
- **id**: адрес аккаунта
- **balance**: баланс
- **updatedAt**: время обновления

## 5. Текущие ограничения и временные решения

1. **Ограниченная глубина исторических данных**:
   - Сейчас мы начинаем индексацию с текущего блока минус небольшой отступ (`DEPTH = 50`)
   - В будущем может потребоваться возможность индексации с произвольной точки в прошлом

2. **Оптимизация удаления**:
   - Текущая реализация не использует пакетное удаление
   - В будущем можно оптимизировать для более эффективной работы с большими объемами данных

3. **Отсутствие мониторинга и метрик**:
   - Сейчас используется только базовое логирование через консоль
   - В будущем стоит добавить полноценный мониторинг и метрики

4. **Обработка реорганизации цепи (forks)**:
   - Сейчас отсутствует полноценная обработка форков
   - В будущем нужно добавить механизм обработки chain reorganization

5. **Улучшенное управление ошибками**:
   - Сейчас ошибки просто логируются
   - В будущем нужен более продвинутый механизм обработки ошибок с возможностью восстановления

## 6. Запуск и управление

### Основные команды:

```bash
# Сборка проекта
npm run build

# Применение миграций базы данных
npm run migration:apply

# Запуск процессора
npm run process

# Комбинированная команда (сборка + миграции + запуск)
npm run start
```

## 7. Следующие шаги

1. Оптимизация производительности индексатора
2. Реализация обработки реорганизации цепи (forks)
3. Добавление дополнительных типов событий для индексации
4. Интеграция с Hasura GraphQL для предоставления API
5. Внедрение мониторинга и метрик
6. Разработка фронтенда для отображения данных 